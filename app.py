import streamlit as st
import os
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI
from langchain_openai import ChatOpenAI
import tempfile
import shutil
import time
from datetime import datetime

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ë‚˜ë§Œì˜ RAG ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

# CSS ìŠ¤íƒ€ì¼ë§
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 10px;
        margin-bottom: 2rem;
        text-align: center;
    }
    
    .main-header h1 {
        color: white;
        margin: 0;
        font-size: 2.5rem;
    }
    
    .main-header p {
        color: #f0f0f0;
        margin: 0.5rem 0 0 0;
        font-size: 1.1rem;
    }
    
    .chat-message {
        padding: 1rem;
        border-radius: 10px;
        margin: 1rem 0;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    
    .user-message {
        background-color: #e3f2fd;
        border-left: 4px solid #2196f3;
    }
    
    .assistant-message {
        background-color: #f3e5f5;
        border-left: 4px solid #9c27b0;
    }
    
    .upload-section {
        background-color: #f8f9fa;
        padding: 1.5rem;
        border-radius: 10px;
        border: 2px dashed #dee2e6;
    }
    
    .status-success {
        background-color: #d4edda;
        color: #155724;
        padding: 0.75rem;
        border-radius: 5px;
        border: 1px solid #c3e6cb;
    }
    
    .status-error {
        background-color: #f8d7da;
        color: #721c24;
        padding: 0.75rem;
        border-radius: 5px;
        border: 1px solid #f5c6cb;
    }
</style>
""", unsafe_allow_html=True)

class RAGChatbot:
    def __init__(self):
        self.vectorstore = None
        self.qa_chain = None
        
    def load_documents(self, uploaded_files):
        """ì—…ë¡œë“œëœ ë¬¸ì„œë“¤ì„ ë¡œë“œí•˜ê³  ì²˜ë¦¬"""
        documents = []
        
        for uploaded_file in uploaded_files:
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                tmp_file.write(uploaded_file.getvalue())
                tmp_file_path = tmp_file.name
            
            try:
                # PDF ë¡œë”ë¡œ ë¬¸ì„œ ë¡œë“œ
                loader = PyPDFLoader(tmp_file_path)
                docs = loader.load()
                documents.extend(docs)
            except Exception as e:
                st.error(f"ë¬¸ì„œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            finally:
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                os.unlink(tmp_file_path)
        
        return documents
    
    def create_vectorstore(self, documents):
        """ë¬¸ì„œë“¤ì„ ë²¡í„°ìŠ¤í† ì–´ë¡œ ë³€í™˜"""
        if not documents:
            return None
            
        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        texts = text_splitter.split_documents(documents)
        
        # ì„ë² ë”© ìƒì„±
        embeddings = OpenAIEmbeddings()
        
        # FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        vectorstore = FAISS.from_documents(texts, embeddings)
        
        return vectorstore
    
    def create_qa_chain(self, vectorstore):
        """ì§ˆì˜ì‘ë‹µ ì²´ì¸ ìƒì„±"""
        if vectorstore is None:
            return None
            
        llm = ChatOpenAI(temperature=0.7, model_name="gpt-3.5-turbo")
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vectorstore.as_retriever(search_kwargs={"k": 3})
        )
        
        return qa_chain
    
    def query(self, question):
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"""
        if self.qa_chain is None:
            return "ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
        
        try:
            response = self.qa_chain.run(question)
            return response
        except Exception as e:
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def main():
    # ë©”ì¸ í—¤ë”
    st.markdown("""
    <div class="main-header">
        <h1>ğŸ¤– ë‚˜ë§Œì˜ RAG ì±—ë´‡</h1>
        <p>ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  AIì™€ ëŒ€í™”í•´ë³´ì„¸ìš”!</p>
    </div>
    """, unsafe_allow_html=True)
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    with st.sidebar:
        st.markdown("### âš™ï¸ ì„¤ì •")
        
        # API í‚¤ ì…ë ¥
        api_key = st.text_input("OpenAI API í‚¤", type="password", help="OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
        
        if api_key:
            os.environ["OPENAI_API_KEY"] = api_key
        
        st.markdown("---")
        
        # ë‹¤í¬ ëª¨ë“œ í† ê¸€
        dark_mode = st.checkbox("ğŸŒ™ ë‹¤í¬ ëª¨ë“œ", value=False)
        
        # ì„¸ì…˜ ì´ˆê¸°í™” ë²„íŠ¼
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True):
            st.session_state.messages = []
            st.session_state.vectorstore = None
            st.session_state.qa_chain = None
            st.rerun()
        
        st.markdown("---")
        
        # ì•± ì •ë³´
        st.markdown("### ğŸ“± ì•± ì •ë³´")
        st.markdown("""
        **ë²„ì „**: 1.0.0  
        **ê°œë°œì**: í•™ìƒ  
        **ê¸°ìˆ **: Langchain + Streamlit
        """)
    
    # ë©”ì¸ ì˜ì—­ì„ ë‘ ê°œì˜ ì»¬ëŸ¼ìœ¼ë¡œ ë¶„í• 
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.markdown("### ğŸ“„ ë¬¸ì„œ ì—…ë¡œë“œ")
        
        # ì—…ë¡œë“œ ì„¹ì…˜
        st.markdown('<div class="upload-section">', unsafe_allow_html=True)
        
        # íŒŒì¼ ì—…ë¡œë“œ
        uploaded_files = st.file_uploader(
            "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
            type=['pdf'],
            accept_multiple_files=True,
            help="ë¶„ì„í•  PDF ë¬¸ì„œë“¤ì„ ì„ íƒí•˜ì„¸ìš”"
        )
        
        st.markdown('</div>', unsafe_allow_html=True)
        
        # ë¬¸ì„œ ì²˜ë¦¬ ë²„íŠ¼
        if st.button("ğŸ“š ë¬¸ì„œ ë¶„ì„ ì‹œì‘", disabled=not uploaded_files, use_container_width=True):
            if not api_key:
                st.markdown('<div class="status-error">OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.</div>', unsafe_allow_html=True)
            else:
                with st.spinner("ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    chatbot = RAGChatbot()
                    
                    # ë¬¸ì„œ ë¡œë“œ
                    documents = chatbot.load_documents(uploaded_files)
                    
                    if documents:
                        # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
                        vectorstore = chatbot.create_vectorstore(documents)
                        
                        # QA ì²´ì¸ ìƒì„±
                        qa_chain = chatbot.create_qa_chain(vectorstore)
                        
                        # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                        st.session_state.vectorstore = vectorstore
                        st.session_state.qa_chain = qa_chain
                        
                        st.markdown(f'<div class="status-success">âœ… {len(documents)}ê°œì˜ ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤!</div>', unsafe_allow_html=True)
                    else:
                        st.markdown('<div class="status-error">ë¬¸ì„œë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.</div>', unsafe_allow_html=True)
        
        # ë¬¸ì„œ ìƒíƒœ í‘œì‹œ
        if st.session_state.get("qa_chain"):
            st.markdown("### âœ… ë¬¸ì„œ ìƒíƒœ")
            st.success("ë¬¸ì„œê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
            st.markdown("ì´ì œ ì§ˆë¬¸ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    with col2:
        st.markdown("### ğŸ’¬ ì±„íŒ…")
        
        # ì±„íŒ… ë©”ì‹œì§€ ì´ˆê¸°í™”
        if "messages" not in st.session_state:
            st.session_state.messages = []
        
        # ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
        for message in st.session_state.messages:
            if message["role"] == "user":
                st.markdown(f'<div class="chat-message user-message"><strong>ğŸ‘¤ ì‚¬ìš©ì:</strong><br>{message["content"]}</div>', unsafe_allow_html=True)
            else:
                st.markdown(f'<div class="chat-message assistant-message"><strong>ğŸ¤– ì±—ë´‡:</strong><br>{message["content"]}</div>', unsafe_allow_html=True)
        
        # ì‚¬ìš©ì ì…ë ¥
        if prompt := st.chat_input("ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”..."):
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            st.session_state.messages.append({"role": "user", "content": prompt})
            
            # ì±—ë´‡ ì‘ë‹µ ìƒì„±
            if st.session_state.get("qa_chain"):
                chatbot = RAGChatbot()
                chatbot.qa_chain = st.session_state.qa_chain
                
                with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    response = chatbot.query(prompt)
                    st.session_state.messages.append({"role": "assistant", "content": response})
                    st.rerun()
            else:
                st.error("ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ë¶„ì„í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()